{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CDE_Extended.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swapon15/machine_learning/blob/master/CDE_Extended.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "y6DqkWZKE13o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Install PyDrive and import necessary modules to download files from GDrive\n",
        "Source:Code Snippets - Downloading files or importing data from google drive"
      ]
    },
    {
      "metadata": {
        "id": "w3OaMeOIA2JX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ATZ8uGp0FDfW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Downloading the files"
      ]
    },
    {
      "metadata": {
        "id": "K4EeLwXeEUbV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download file(s) based on file ID.\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "file_id_1 = '15ztdu7YfMJVvAX3nLQrl9w8VPZolJxMU' #data.py\n",
        "file_id_2 = '1WbFNmolzLI3GjQ1MFcNEh_r7r6ahSWir' #constant.py\n",
        "#file_id_3 = '1Wy_R-Ao93mvn91AgkyJLw_7tqpCDbKcE' #doTest.py\n",
        "file_id_4 = '1yKMFBNXTGggA2NdCQAWZA-MGV_N-QoID' #sparsity.py\n",
        "file_id_5 = '1jcASbHMOEExGDsz1LsBSs8kGY8dJG2z8' #myDataset.py\n",
        "file_ids = [file_id_1,file_id_2,file_id_4, file_id_5]\n",
        "all_modules = []\n",
        "for id in file_ids:\n",
        "    all_modules.append(drive.CreateFile({'id': id}))\n",
        "file_names = ['data.py', 'constant.py','sparsity.py', 'myDataset.py']\n",
        "for i in range(len(all_modules)):\n",
        "    all_modules[i].GetContentFile(file_names[i])\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i5OE3OIxNeGW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Import data modules and necessary sklearn module"
      ]
    },
    {
      "metadata": {
        "id": "qLoWTDcPFGqf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import data as pdbc\n",
        "import constant\n",
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w4Kf1X80Og2u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Splitting the test solutions randomly into train and test"
      ]
    },
    {
      "metadata": {
        "id": "YfwgNe5Ba7Og",
        "colab_type": "code",
        "outputId": "8862b0d7-f2cd-45cf-8f97-8703caeec588",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "%%writefile splitter.py\n",
        "def split_test_solutions(test_solutions, split_ratio=0.40):\n",
        "  \"\"\"\n",
        "  Spliter of test solutions.\n",
        "  It splits the test solutions by given split_ratio. Separates \n",
        "  test solutions randomly into two subsets. One subset is for training\n",
        "  and the other one is for testing.\n",
        "  \n",
        "  Parameters:\n",
        "  test_solution (dictionary): each key is a tuple of features e.g. (1, 2, 10, 1)\n",
        "                               for a two dimensional number games. Here, 1, 2 is \n",
        "                               a number and 10, 1 is another number. \n",
        "                               \n",
        "                               each value is a tuple of win/loss and respective index/\n",
        "                               position numbers of the key value. e.g., (True, 9999)\n",
        "                               \n",
        "  split_ratio        (float):  the ratio for splitting test solutions. The solutions are\n",
        "                               splitted into split_ratio * 100 for trainin and (1 - split_ratio) * 100\n",
        "                               for testing.\n",
        "  \n",
        "  Returns            (tuple):  a tuple of training and test solutions. \n",
        "  \n",
        "  \"\"\"\n",
        "  training_points = np.floor(split_ratio * len(test_solutions))\n",
        "  selected_points = []\n",
        "  training_set, test_set = {}, {}\n",
        "  \n",
        "  while len(selected_points) < training_points:\n",
        "    for features, labels in test_solutions.items():\n",
        "      random_position = np.random.random_integers(0, training_points - 1)\n",
        "      if random_position not in selected_points:\n",
        "        training_set[features] = labels\n",
        "        selected_points.append(random_position)\n",
        "      else:\n",
        "        test_set[features] = labels\n",
        "\n",
        "  return (training_set, test_set)\n",
        "\n",
        "\n",
        "def make_data_compatible(training_solutions, test_solutions):\n",
        "    \"\"\"\n",
        "    Make training and test solutions to be compatible for sklearns\n",
        "    MLPClassifier.\n",
        "    \n",
        "    Parameters:\n",
        "      training_solutions (dict): randomly splitted training data\n",
        "      test_solutions     (dict): randomly splitted test data\n",
        "    Returs:\n",
        "          X_train  (list): all the training samples\n",
        "          y_train  (list): respective features for the training samples\n",
        "          X_test   (list): all the  test samples\n",
        "          y_test   (list): respective features for the test samples \n",
        "    \n",
        "    \"\"\"\n",
        "    X_train, y_train, X_test, y_test = [], [], [], []\n",
        "\n",
        "    for features, labels in training_solutions.items():\n",
        "        X_train.append(list(features))\n",
        "        y_train.append(labels[0])\n",
        "\n",
        "    for features, labels in test_solutions.items():\n",
        "        X_test.append(list(features))\n",
        "        y_test.append(labels[0])\n",
        "    return (X_train, y_train, X_test, y_test)\n",
        "\n",
        "\n",
        "\n",
        "def get_data(amount):\n",
        "    \"\"\"\n",
        "    Calls several data producer and splitter for test solutions.\n",
        "  \n",
        "    Parameters: amount (float): ratio of splitting the test solutions\n",
        "  \n",
        "    Returns: two subsets created from original dataset of test solutions\n",
        "    \"\"\"\n",
        "    all_test_solutions, no_need = pdbc.getSyntheticData(constant.COMP_ON_ONE)\n",
        "    training_data, test_data = split_test_solutions(all_test_solutions, amount) # random split of data by 40%\n",
        "    return make_data_compatible(training_data, test_data)\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting splitter.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UzgCvfztejwo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## A Neural Network with One hidden layer "
      ]
    },
    {
      "metadata": {
        "id": "gdyKBbuuehoZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ab662ca6-904d-450f-bf50-bc65c676ac3c"
      },
      "cell_type": "code",
      "source": [
        "%%writefile neural.py\n",
        "def build_MLP():\n",
        "    \"\"\"\n",
        "    Sets the parameters of learning process of a neural network.\n",
        "    Parameters: No parameter\n",
        "    Returns   : a mlp instance\n",
        "    \"\"\"\n",
        "    mlp = MLPClassifier(\n",
        "                  hidden_layer_sizes = (10, ), # one hidden layer with 10 nodes\n",
        "                  max_iter=100,\n",
        "                  alpha=1e-4,\n",
        "                  solver='lbfgs',\n",
        "                  tol=1e-4,\n",
        "                  random_state=1\n",
        "                  )\n",
        "    return mlp\n",
        "\n",
        "def get_MLP_test_accuracy(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    train the model, test it and returns test accuracy\n",
        "    Parameters:\n",
        "    X_train  (list): all the training samples\n",
        "    y_train  (list): respective features for the training samples\n",
        "    X_test   (list): all the  test samples\n",
        "    y_test   (list): respective features for the test samples\n",
        "    \n",
        "    Returns:\n",
        "    accuracy (float): test accuracy of the model\n",
        "    \"\"\"\n",
        "    mlp = build_MLP()\n",
        "    mlp.fit(X_train, y_train)\n",
        "    accuracy = mlp.score(X_test, y_test)\n",
        "    return accuracy\n",
        "\n",
        "def get_MLP_prediction(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    train the model and predict on t test data. Then retuns the predicted\n",
        "    labels \n",
        "    Parameters:\n",
        "    X_train  (list): all the training samples\n",
        "    y_train  (list): respective features for the training samples\n",
        "    X_test   (list): all the  test samples\n",
        "    y_test   (list): respective features for the test samples\n",
        "    \n",
        "    Returns:\n",
        "    predicted_lable (list): labels of the prediction\n",
        "    \"\"\"\n",
        "    mlp = build_MLP()\n",
        "    mlp.fit(X_train, y_train)\n",
        "    predicted_label = mlp.predict(X_test)\n",
        "    return predicted_label"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting neural.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "y0DUGZgTehHI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Client code for data splitter and neural network "
      ]
    },
    {
      "metadata": {
        "id": "L4nK3dG-OfZO",
        "colab_type": "code",
        "outputId": "797e85c8-769b-4556-9337-fc39501e5e40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "exec(open('splitter.py').read())\n",
        "exec(open('neural.py').read())\n",
        "\n",
        "amount = 0.40\n",
        "X_train, y_train, X_test, y_test = get_data(amount)\n",
        "predicted_labels = get_MLP_prediction(X_train, y_train, X_test, y_test)\n",
        "print(predicted_labels)\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: DeprecationWarning: This function is deprecated. Please call randint(0, 3999.0 + 1) instead\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[False False False ... False False  True]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}